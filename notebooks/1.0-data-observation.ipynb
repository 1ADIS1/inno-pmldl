{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and display the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.065473</td>\n",
       "      <td>0.999039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.994215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.999348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>You didn't know that Estelle had stolen some f...</td>\n",
       "      <td>you didn't know that Estelle stole your fish f...</td>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.949143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>It'il suck the life out of you!</td>\n",
       "      <td>you'd be sucked out of your life!</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>I can't fuckin' take that, bruv.</td>\n",
       "      <td>I really can't take this.</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>They called me a fucking hero. The truth is I ...</td>\n",
       "      <td>they said I was a hero, but I didn't care.</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>I did not screw him.</td>\n",
       "      <td>I didn't fuck him.</td>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.009480</td>\n",
       "      <td>0.994174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577777 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "0       If Alkar is flooding her with psychic waste, t...   \n",
       "1                               Now you're getting nasty.   \n",
       "2                Well, we could spare your life, for one.   \n",
       "3               Ah! Monkey, you've got to snap out of it.   \n",
       "4                        I've got orders to put her down.   \n",
       "...                                                   ...   \n",
       "577772  You didn't know that Estelle had stolen some f...   \n",
       "577773                    It'il suck the life out of you!   \n",
       "577774                   I can't fuckin' take that, bruv.   \n",
       "577775  They called me a fucking hero. The truth is I ...   \n",
       "577776                               I did not screw him.   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "0       if Alkar floods her with her mental waste, it ...    0.785171   \n",
       "1                             you're becoming disgusting.    0.749687   \n",
       "2                           well, we can spare your life.    0.919051   \n",
       "3                            monkey, you have to wake up.    0.664333   \n",
       "4                              I have orders to kill her.    0.726639   \n",
       "...                                                   ...         ...   \n",
       "577772  you didn't know that Estelle stole your fish f...    0.870322   \n",
       "577773                  you'd be sucked out of your life!    0.722897   \n",
       "577774                          I really can't take this.    0.617511   \n",
       "577775         they said I was a hero, but I didn't care.    0.679613   \n",
       "577776                                 I didn't fuck him.    0.868475   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "0          0.010309  0.014195  0.981983  \n",
       "1          0.071429  0.065473  0.999039  \n",
       "2          0.268293  0.213313  0.985068  \n",
       "3          0.309524  0.053362  0.994215  \n",
       "4          0.181818  0.009402  0.999348  \n",
       "...             ...       ...       ...  \n",
       "577772     0.030769  0.000121  0.949143  \n",
       "577773     0.058824  0.996124  0.215794  \n",
       "577774     0.212121  0.984538  0.000049  \n",
       "577775     0.358209  0.991945  0.000124  \n",
       "577776     0.095238  0.009480  0.994174  \n",
       "\n",
       "[577777 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# I am using double space separator to parse data correctly.\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer=\"D:/Py_Projects/Inno-PMLDL/data/raw/filtered.tsv\", sep='\t', header=0)\n",
    "\n",
    "# Dropping unnecessary column with row indices because they are already included in pandas DataFrame.\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As was said in the assignment task, it is better for us to find pairs, where reference text has high toxicity level, and translation - low.\n",
    "### We should be careful because there are references with low toxicity and translations with high toxicities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find pairs with ref_tox > trn_tox (for now). In the future, we can also try to compare toxicities difference with another small value to remove similar toxicities from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm not gonna have a child... ...with the same...</td>\n",
       "      <td>I'm not going to breed kids with a genetic dis...</td>\n",
       "      <td>0.703185</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.950956</td>\n",
       "      <td>0.035846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>They're all laughing at us, so we'll kick your...</td>\n",
       "      <td>they're laughing at us. We'll show you.</td>\n",
       "      <td>0.618866</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maine was very short on black people back then.</td>\n",
       "      <td>there wasn't much black in Maine then.</td>\n",
       "      <td>0.720482</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.963680</td>\n",
       "      <td>0.148710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>So now their spirits are cursed, walking back ...</td>\n",
       "      <td>their souls are cursed, they guard the paths, ...</td>\n",
       "      <td>0.755883</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.143992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Come on, Cal, leave that shit alone.</td>\n",
       "      <td>come on, Cal, put it down.</td>\n",
       "      <td>0.660481</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577770</th>\n",
       "      <td>I am so crazy nuts about you guys.</td>\n",
       "      <td>I'm so crazy about you guys.</td>\n",
       "      <td>0.934512</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.973442</td>\n",
       "      <td>0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577771</th>\n",
       "      <td>I thought American men were bad enough, but no...</td>\n",
       "      <td>an American man is worth nothing, but for you,...</td>\n",
       "      <td>0.671444</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>0.035941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>It'il suck the life out of you!</td>\n",
       "      <td>you'd be sucked out of your life!</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>I can't fuckin' take that, bruv.</td>\n",
       "      <td>I really can't take this.</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>They called me a fucking hero. The truth is I ...</td>\n",
       "      <td>they said I was a hero, but I didn't care.</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319142 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "5       I'm not gonna have a child... ...with the same...   \n",
       "6       They're all laughing at us, so we'll kick your...   \n",
       "7         Maine was very short on black people back then.   \n",
       "11      So now their spirits are cursed, walking back ...   \n",
       "13                   Come on, Cal, leave that shit alone.   \n",
       "...                                                   ...   \n",
       "577770                 I am so crazy nuts about you guys.   \n",
       "577771  I thought American men were bad enough, but no...   \n",
       "577773                    It'il suck the life out of you!   \n",
       "577774                   I can't fuckin' take that, bruv.   \n",
       "577775  They called me a fucking hero. The truth is I ...   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "5       I'm not going to breed kids with a genetic dis...    0.703185   \n",
       "6                 they're laughing at us. We'll show you.    0.618866   \n",
       "7                  there wasn't much black in Maine then.    0.720482   \n",
       "11      their souls are cursed, they guard the paths, ...    0.755883   \n",
       "13                             come on, Cal, put it down.    0.660481   \n",
       "...                                                   ...         ...   \n",
       "577770                       I'm so crazy about you guys.    0.934512   \n",
       "577771  an American man is worth nothing, but for you,...    0.671444   \n",
       "577773                  you'd be sucked out of your life!    0.722897   \n",
       "577774                          I really can't take this.    0.617511   \n",
       "577775         they said I was a hero, but I didn't care.    0.679613   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "5          0.206522  0.950956  0.035846  \n",
       "6          0.230769  0.999492  0.000131  \n",
       "7          0.187500  0.963680  0.148710  \n",
       "11         0.013245  0.842509  0.143992  \n",
       "13         0.270270  0.999637  0.000279  \n",
       "...             ...       ...       ...  \n",
       "577770     0.171429  0.973442  0.000709  \n",
       "577771     0.371212  0.999624  0.035941  \n",
       "577773     0.058824  0.996124  0.215794  \n",
       "577774     0.212121  0.984538  0.000049  \n",
       "577775     0.358209  0.991945  0.000124  \n",
       "\n",
       "[319142 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[df['ref_tox'] > df['trn_tox']]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, obviously, we need to perform text cleaning to reduce the data dimensionality that includes:\n",
    "### lowercasing, removing punctuation, stop words, and numbers, tokenizing, and stemming both reference and translation columns.\n",
    "\n",
    "### For this, I will use the code from the labs and slightly change it for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def lower_text(text: str):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text: str):\n",
    "    \"\"\"\n",
    "    Substitute all punctuations with space in case of\n",
    "    \"there is5dogs\".\n",
    "    \n",
    "    If subs with '' -> \"there isdogs\"\n",
    "    With ' ' -> there is dogs\n",
    "    \"\"\"\n",
    "    text_nonum = re.sub(r'\\d+', ' ', text)\n",
    "    return text_nonum\n",
    "\n",
    "def remove_punctuation(text: str):\n",
    "    \"\"\"\n",
    "    Substitute all punctiations with space in case of\n",
    "    \"hello!nice to meet you\"\n",
    "    \n",
    "    If subs with '' -> \"hellonice to meet you\"\n",
    "    With ' ' -> \"hello nice to meet you\"\n",
    "    \"\"\"\n",
    "    text_nopunct = re.sub(r'[^a-z|\\s]+', ' ', text)\n",
    "    return text_nopunct\n",
    "\n",
    "def remove_multiple_spaces(text: str):\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text).strip()\n",
    "    return text_no_doublespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test lowercasing, numbers, punctuation, and multiple spaces removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text:\n",
      " If Alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.\n",
      "----------\n",
      "Lowercased:\n",
      " if alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.\n",
      "----------\n",
      "Removed numbers:\n",
      " if alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.\n",
      "----------\n",
      "Removed punctuation:\n",
      " if alkar is flooding her with psychic waste  that explains the high level of neurotransmitters \n",
      "----------\n",
      "Removed mult spaces:\n",
      " if alkar is flooding her with psychic waste that explains the high level of neurotransmitters\n"
     ]
    }
   ],
   "source": [
    "sample_text = df.reference[0]\n",
    "\n",
    "_lowered = lower_text(sample_text)\n",
    "_without_numbers = remove_numbers(_lowered)\n",
    "_without_punct = remove_punctuation(_without_numbers)\n",
    "_single_spaced = remove_multiple_spaces(_without_punct)\n",
    "\n",
    "print(\"Sample text:\\n\", sample_text)\n",
    "print('-'*10)\n",
    "print(\"Lowercased:\\n\", _lowered)\n",
    "print('-'*10)\n",
    "print(\"Removed numbers:\\n\", _without_numbers)\n",
    "print('-'*10)\n",
    "print(\"Removed punctuation:\\n\", _without_punct)\n",
    "print('-'*10)\n",
    "print(\"Removed mult spaces:\\n\", _single_spaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download nltk packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(tokenized_text: list[str]) -> list[str]:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokenized_text if word not in stop_words]\n",
    "\n",
    "\n",
    "# TODO: Produces weird words, consider replacing with lemmatisation\n",
    "def stem_words(tokenized_text: list[str]) -> list[str]:\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(plural) for plural in tokenized_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tokenisation, stop words removal, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous text:\n",
      " if alkar is flooding her with psychic waste that explains the high level of neurotransmitters\n",
      "----------\n",
      "Tokenised:\n",
      " ['if', 'alkar', 'is', 'flooding', 'her', 'with', 'psychic', 'waste', 'that', 'explains', 'the', 'high', 'level', 'of', 'neurotransmitters']\n",
      "----------\n",
      "Without stop words:\n",
      " ['alkar', 'flooding', 'psychic', 'waste', 'explains', 'high', 'level', 'neurotransmitters']\n",
      "----------\n",
      "Stemmed:\n",
      " ['alkar', 'flood', 'psychic', 'wast', 'explain', 'high', 'level', 'neurotransmitt']\n"
     ]
    }
   ],
   "source": [
    "_tokenized = tokenize_text(_single_spaced)\n",
    "_without_sw = remove_stop_words(_tokenized)\n",
    "_stemmed = stem_words(_without_sw)\n",
    "\n",
    "print(\"Previous text:\\n\", _single_spaced)\n",
    "print('-'*10)\n",
    "print(\"Tokenised:\\n\", _tokenized)\n",
    "print('-'*10)\n",
    "print(\"Without stop words:\\n\", _without_sw)\n",
    "print('-'*10)\n",
    "print(\"Stemmed:\\n\", _stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_stage(text):\n",
    "    _lowered = lower_text(text)\n",
    "    _without_numbers = remove_numbers(_lowered)\n",
    "    _without_punct = remove_punctuation(_without_numbers)\n",
    "    _single_spaced = remove_multiple_spaces(_without_punct)\n",
    "    _tokenized = tokenize_text(_single_spaced)\n",
    "    _without_sw = remove_stop_words(_tokenized)\n",
    "    _stemmed = stem_words(_without_sw)\n",
    "    \n",
    "    return _stemmed\n",
    "\n",
    "def clean_text(df):\n",
    "    df['reference'] = df['reference'].apply(preprocessing_stage)\n",
    "    df['translation'] = df['translation'].apply(preprocessing_stage)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alifa\\AppData\\Local\\Temp\\ipykernel_10348\\3258986215.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['reference'] = df['reference'].apply(preprocessing_stage)\n",
      "C:\\Users\\alifa\\AppData\\Local\\Temp\\ipykernel_10348\\3258986215.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['translation'] = df['translation'].apply(preprocessing_stage)\n"
     ]
    }
   ],
   "source": [
    "df_train_cleaned = clean_text(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[gon, na, child, genet, disord, gon, na, die, l]</td>\n",
       "      <td>[go, breed, kid, genet, disord, make, die]</td>\n",
       "      <td>0.703185</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.950956</td>\n",
       "      <td>0.035846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[laugh, us, kick, ass]</td>\n",
       "      <td>[laugh, us, show]</td>\n",
       "      <td>0.618866</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.999492</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[main, short, black, peopl, back]</td>\n",
       "      <td>[much, black, main]</td>\n",
       "      <td>0.720482</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.963680</td>\n",
       "      <td>0.148710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[spirit, curs, walk, back, road, waterway, fin...</td>\n",
       "      <td>[soul, curs, guard, path, say, encount, unfait...</td>\n",
       "      <td>0.755883</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.842509</td>\n",
       "      <td>0.143992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[come, cal, leav, shit, alon]</td>\n",
       "      <td>[come, cal, put]</td>\n",
       "      <td>0.660481</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577770</th>\n",
       "      <td>[crazi, nut, guy]</td>\n",
       "      <td>[crazi, guy]</td>\n",
       "      <td>0.934512</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.973442</td>\n",
       "      <td>0.000709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577771</th>\n",
       "      <td>[thought, american, men, bad, enough, none, ev...</td>\n",
       "      <td>[american, man, worth, noth, feel, like, helpl...</td>\n",
       "      <td>0.671444</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>0.035941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>[il, suck, life]</td>\n",
       "      <td>[suck, life]</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>[fuckin, take, bruv]</td>\n",
       "      <td>[realli, take]</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>[call, fuck, hero, truth, care, anymor]</td>\n",
       "      <td>[said, hero, care]</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319142 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "5        [gon, na, child, genet, disord, gon, na, die, l]   \n",
       "6                                  [laugh, us, kick, ass]   \n",
       "7                       [main, short, black, peopl, back]   \n",
       "11      [spirit, curs, walk, back, road, waterway, fin...   \n",
       "13                          [come, cal, leav, shit, alon]   \n",
       "...                                                   ...   \n",
       "577770                                  [crazi, nut, guy]   \n",
       "577771  [thought, american, men, bad, enough, none, ev...   \n",
       "577773                                   [il, suck, life]   \n",
       "577774                               [fuckin, take, bruv]   \n",
       "577775            [call, fuck, hero, truth, care, anymor]   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "5              [go, breed, kid, genet, disord, make, die]    0.703185   \n",
       "6                                       [laugh, us, show]    0.618866   \n",
       "7                                     [much, black, main]    0.720482   \n",
       "11      [soul, curs, guard, path, say, encount, unfait...    0.755883   \n",
       "13                                       [come, cal, put]    0.660481   \n",
       "...                                                   ...         ...   \n",
       "577770                                       [crazi, guy]    0.934512   \n",
       "577771  [american, man, worth, noth, feel, like, helpl...    0.671444   \n",
       "577773                                       [suck, life]    0.722897   \n",
       "577774                                     [realli, take]    0.617511   \n",
       "577775                                 [said, hero, care]    0.679613   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "5          0.206522  0.950956  0.035846  \n",
       "6          0.230769  0.999492  0.000131  \n",
       "7          0.187500  0.963680  0.148710  \n",
       "11         0.013245  0.842509  0.143992  \n",
       "13         0.270270  0.999637  0.000279  \n",
       "...             ...       ...       ...  \n",
       "577770     0.171429  0.973442  0.000709  \n",
       "577771     0.371212  0.999624  0.035941  \n",
       "577773     0.058824  0.996124  0.215794  \n",
       "577774     0.212121  0.984538  0.000049  \n",
       "577775     0.358209  0.991945  0.000124  \n",
       "\n",
       "[319142 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
